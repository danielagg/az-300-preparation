Data FActory
- data movement
- data transformation

1) do transformation ourselves within Data Factory
2) call another service to do that on ADF's behalf


SQL datawarehouse: concurrent connections can be an issue (100 or so)
azure synapse analytics: 10,000 concurrent connections

---------------

COMPONENTS:
linked service: ADF connect to data stores, or compute resources

Dataset: we define this, the end result

Activity: performs action on data, feeds the dataset
- usually there's more than 1


Pipeline: activities grouped together
- trigger, schedule this


Parameters
Integration runtime
Control flow

---------------

We could do:
- on prem SQL server -> ADF to ingest data -> Data Lake -> Synapse Analytics
- on prem SQL server -> ADF to ingest data -> Synapse Analytics (we can skip data lake)


Copy activity: grab data from the source, do some business logic if necessary and dump it into the sink (destination)
- we can use auto Integration Runtime (it discovers whether we are doing cloud to sink or onprem to sink)

NOTE: preprocess data before doing this, because:
- compressed data files is more efficient (no serialization/deserialization)


